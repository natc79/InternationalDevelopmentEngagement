{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Wikipedia to Obtain Topic Key Words\n",
    "\n",
    "This code downloads wikipedia pages, processes them and obtains the top 20 most frequent nouns and top 10 most frequent bigram words with at least one word being a noun.  This data is output into files that can later be used to tag subject topics.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request as urlrequest\n",
    "import wikipedia as wiki\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#subjects and search terms\n",
    "subjects = {'aid_effectiveness' : ['aid effectiveness'],\n",
    "            'climate_change' : ['climate change'],\n",
    "            'corruption_governance' : ['corruption','corruption and transparency','governance'],\n",
    "            'data_technology' : ['data and technology'],\n",
    "            'education' : ['education'],\n",
    "            'finance_investment' : ['finance','investment'],\n",
    "            'gender' : ['gender'],\n",
    "            'global_health' : ['global health'],\n",
    "            'infrastructure' : ['infrastructure'],\n",
    "            #'interntl_inst' : ['international institutions'],\n",
    "            'migration' : ['human migration'],\n",
    "            'povertyineqgrowth' : ['poverty','inequality and growth'],\n",
    "            'intltrade' : ['international trade']\n",
    "}\n",
    "\n",
    "def get_wikipedia_pages(subjects):\n",
    "\n",
    "    #subjects is a dictionary pointing to list of search terms\n",
    "    for key, val in subjects.items():\n",
    "        text = \"\"\n",
    "        for search_term in val:\n",
    "            # 1. Grab  the list from wikipedia.\n",
    "            wiki.set_lang('en')\n",
    "            searches = wiki.search(search_term)\n",
    "            print(wiki.search(search_term))\n",
    "            #get the first two pages and concatenate or save these pages in folder\n",
    "            for i in range(0,2):\n",
    "                fname = re.sub('\\s',\"_\",searches[i])\n",
    "                s = wiki.page(searches[i])\n",
    "                #fname = re.sub('\\s',\"_\",searches[i])\n",
    "                print(fname)\n",
    "                html = urlrequest.urlopen(s.url).read()\n",
    "                parser = BeautifulSoup(html, 'html.parser')\n",
    "                text = text + parser.get_text()\n",
    "  \n",
    "        #Extract key words and replace with space\n",
    "        text = re.sub('\\W+', \" \", text.lower())\n",
    "        # Replace any non-letter, space, or digit character in the headlines.\n",
    "        text = re.sub(r'[\\d]',\"\",text)\n",
    "        text = re.sub('\\s+',\" \",text)\n",
    "\n",
    "        #place things into parts of speech and label as noun, verb etc.\n",
    "        tokenized = nltk.tokenize.word_tokenize(text)\n",
    "        pos_words = set(nltk.pos_tag(tokenized))\n",
    "    \n",
    "        #use the built in list of stop words \n",
    "        from nltk.corpus import stopwords\n",
    "        stop = set(stopwords.words('english'))\n",
    "        more_stop_words = ['articles','countries','country','effects','mediawiki','template','value','limit','information',\n",
    "                          'january','october','wikipedia']\n",
    "       \n",
    "        #keep words that have length <= 4 (did this by lower length thresshold and checking for each the categories manually)\n",
    "        keep_words = ['aid','aid','anti','bank','boy','boys','fund','gas','girl','homo','law','men','ipcc','oecd','poor','risk','role','sea','sex','sub']\n",
    "        clean_split = [w for w in text.split() if w not in stop and w not in more_stop_words and (len(w) >= 5 or w in keep_words)]\n",
    "        #print(set(nltk.pos_tag(clean_split)))        \n",
    "        count_words = pd.Series(clean_split).value_counts()\n",
    "        count_words = count_words[count_words >= 25]\n",
    "        cnt_words = nltk.FreqDist(clean_split)\n",
    "        \n",
    "        NOUNS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "        \n",
    "        most_freq_nouns = [w for w, cnt in cnt_words.most_common(30) if nltk.pos_tag([w])[0][1] in NOUNS]\n",
    "        print(most_freq_nouns)\n",
    "        \n",
    "        #print(type(count_words))\n",
    "        print(len(count_words))\n",
    "\n",
    "        bigrams = list(nltk.bigrams(clean_split))\n",
    "        cnt_bigrams = nltk.FreqDist(bigrams)\n",
    "        most_freq_bigrams = [w for w, cnt in cnt_bigrams.most_common(10) if w[0] in most_freq_nouns or w[1] in most_freq_nouns and w[0] != w[1]]\n",
    "        print(most_freq_bigrams)\n",
    "        count_bigrams = pd.Series(bigrams).value_counts()\n",
    "        count_bigrams = count_bigrams[count_bigrams >= 10]\n",
    "        print(len(count_bigrams))\n",
    "        \n",
    "        fname = 'C:\\\\JobApplications\\\\Facebook\\\\TrainSubjects\\\\'+key+'.csv'\n",
    "        with open(fname, 'wb') as f:\n",
    "            pickle.dump(most_freq_nouns, f)\n",
    "        \n",
    "        fname = 'C:\\\\JobApplications\\\\Facebook\\\\TrainSubjects\\\\'+key+'_bigrams.csv'\n",
    "        with open(fname, 'wb') as f:\n",
    "            pickle.dump(most_freq_bigrams, f)\n",
    "        \n",
    "        #readout the subject files\n",
    "        #most_freq_nouns.to_csv('C:\\\\JobApplications\\\\Facebook\\\\TrainSubjects\\\\'+key+'.csv')\n",
    "        #most_freq_bigrams.to_csv('C:\\\\JobApplications\\\\Facebook\\\\TrainSubjects\\\\'+key+'_bigrams.csv')      \n",
    "        \n",
    "get_wikipedia_pages(subjects)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
